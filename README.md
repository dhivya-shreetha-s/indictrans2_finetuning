# ğŸ¥ Fine-Tuning IndicTrans with LoRA for Healthcare Translation (Tamil â†” English)

## ğŸ“Œ Project Overview
This project focuses on **domain-specific machine translation for healthcare conversations**, addressing the limitations of general-purpose translators in medical contexts.  
A **LoRA-fine-tuned IndicTrans model** is developed to accurately translate **Tamil healthcare sentences into English**, preserving clinical intent and medical terminology.

The system is designed as a **clinical support tool** and includes **confidence-based prediction selection** to ensure safer outputs.

---

## ğŸš€ Key Features
- LoRA fine-tuning on IndicTrans base model  
- Healthcare-specific synthetic dataset  
- Medical dictionary integration for terminology preservation  
- Confidence-scoreâ€“based output selection (LoRA vs Base model)  
- Interactive UI deployed on Hugging Face Spaces  

---

## ğŸ§  Model Architecture
- **Base Model:** ai4bharat/indictrans2-indic-en-1B  
- **Fine-Tuning Method:** LoRA (PEFT)  
- **Tokenizer:** IndicTrans Tokenizer  

> LoRA is **not optional** â€” both base and LoRA models generate predictions, and the final output is selected using a confidence score.

---

## ğŸ“‚ Repository Structure
```
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ huggingface prediction code/
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â””â”€â”€ indictrans-lora-finetuning.ipynb
â”‚
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ medical term + sentences.csv
â”‚   â””â”€â”€ sentences.csv
â”‚
â”œâ”€â”€ Result/
â”‚   â”œâ”€â”€ Training_loss.png
â”‚   â”œâ”€â”€ Error_distribution.png
â”‚   â”œâ”€â”€ Medical_confusion_matrix.png
â”‚   â”œâ”€â”€ Precision_recall_specificity.png
â”‚   â”œâ”€â”€ UI.png
â”‚   â””â”€â”€ Sample prediction/
â”‚       â”œâ”€â”€ Sample_prediction_1.png
â”‚       â”œâ”€â”€ Sample_prediction_2.png
â”‚       â””â”€â”€ Sample_prediction_3.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Description
- Fully **synthetic and anonymized**
- No real patient data used
- Simulates real hospital conversations 

### Dataset Statistics
| Split | Sentences |
|------|-----------|
| Training | 4,92,763 |
| Validation | 27,376 |
| Test | 27,376 |
| **Total** | **5,47,515** |

---

## âš™ï¸ Training Details
- Framework: Hugging Face Transformers  
- Fine-Tuning: LoRA via PEFT  
- Hardware: GPU (CUDA)  
- Loss Function: Cross-Entropy  

---

## ğŸ“ˆ Results & Evaluation
- Improved translation of medical terminology  
- Reduced hallucination  
- Better preservation of clinical meaning  

Evaluation visualizations are available in the `Result/` folder.

---

## ğŸ–¥ï¸ Deployment
- Hosted on **Hugging Face Spaces**
- Built using **Gradio**
- Tamil input â†’ English output
- Medical dictionary loaded securely via environment secrets

---

## ğŸ‘©â€ğŸ’» Author
**Dhivya Shreetha S**  
Mail Id: **dhivyashreetha07@gmail.com** 
